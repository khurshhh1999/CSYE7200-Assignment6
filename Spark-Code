import org.apache.spark.sql.DataFrame

scala> val trainFilePath = "/Users/khursheed/IdeaProjects/CSYE7200/titanic/train.csv"
     | val testFilePath = "/Users/khursheed/IdeaProjects/CSYE7200/titanic/test.csv"
val trainFilePath: String = /Users/khursheed/IdeaProjects/CSYE7200/titanic/train.csv
val testFilePath: String = /Users/khursheed/IdeaProjects/CSYE7200/titanic/test.csv

scala> val trainData: DataFrame = spark.read
     |   .option("header", "true")
     |   .option("inferSchema", "true")
     |   .csv(trainFilePath)
val trainData: org.apache.spark.sql.DataFrame = [PassengerId: int, Survived: int ... 10 more fields]

scala> val testData: DataFrame = spark.read
     |   .option("header", "true")
     |   .option("inferSchema", "true")
     |   .csv(testFilePath)
val testData: org.apache.spark.sql.DataFrame = [PassengerId: int, Pclass: int ... 9 more fields]

scala> trainData.describe().show()
24/03/14 11:58:02 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
+-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+
|summary|      PassengerId|           Survived|            Pclass|                Name|   Sex|               Age|             SibSp|              Parch|            Ticket|             Fare|Cabin|Embarked|
+-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+
|  count|              891|                891|               891|                 891|   891|               714|               891|                891|               891|              891|  204|     889|
|   mean|            446.0| 0.3838383838383838| 2.308641975308642|                NULL|  NULL| 29.69911764705882|0.5230078563411896|0.38159371492704824|260318.54916792738| 32.2042079685746| NULL|    NULL|
| stddev|257.3538420152301|0.48659245426485753|0.8360712409770491|                NULL|  NULL|14.526497332334035|1.1027434322934315| 0.8060572211299488|471609.26868834975|49.69342859718089| NULL|    NULL|
|    min|                1|                  0|                 1|"Andersson, Mr. A...|female|              0.42|                 0|                  0|            110152|              0.0|  A10|       C|
|    max|              891|                  1|                 3|van Melkebeke, Mr...|  male|              80.0|                 8|                  6|         WE/P 5735|         512.3292|    T|       S|
+-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+


scala> trainData.select(trainData.columns.map(c => sum(col(c).isNull.cast("int")).alias(c)): _*).show()
warning: 1 deprecation (since 2.13.0); for details, enable `:setting -deprecation` or `:replay -deprecation`
+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+
|PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|
+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+
|          0|       0|     0|   0|  0|177|    0|    0|     0|   0|  687|       2|
+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+


scala> trainData.groupBy("Survived").count().show()
     | 
+--------+-----+
|Survived|count|
+--------+-----+
|       1|  342|
|       0|  549|
+--------+-----+


scala> trainData.stat.corr("Survived", "Pclass")
     | trainData.stat.corr("Survived", "Age")
val res3: Double = 0.010539215871285682

scala> val trainDataWithFeatures = trainData
     |   .withColumn("FamilySize", col("SibSp") + col("Parch") + 1)
     |   .drop("SibSp", "Parch", "Ticket", "Cabin")
val trainDataWithFeatures: org.apache.spark.sql.DataFrame = [PassengerId: int, Survived: int ... 7 more fields]

scala> val medianAge = trainDataWithFeatures.stat.approxQuantile("Age", Array(0.5), 0.0).head
     | val trainDataFinal = trainDataWithFeatures.na.fill(Map("Age" -> medianAge))
val medianAge: Double = 28.0
val trainDataFinal: org.apache.spark.sql.DataFrame = [PassengerId: int, Survived: int ... 7 more fields]

scala> import org.apache.spark.ml.classification.RandomForestClassifier
     | import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer}
     | import org.apache.spark.ml.{Pipeline, PipelineModel}
     | import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator
import org.apache.spark.ml.classification.RandomForestClassifier
import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer}
import org.apache.spark.ml.{Pipeline, PipelineModel}
import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator

scala> val genderIndexer = new StringIndexer().setInputCol("Sex").setOutputCol("SexIndexed")
     | 
val genderIndexer: org.apache.spark.ml.feature.StringIndexer = strIdx_65d21f17fdbf

scala> val assembler = new VectorAssembler()
     |   .setInputCols(Array("Pclass", "SexIndexed", "Age", "Fare", "FamilySize"))
     |   .setOutputCol("features")
val assembler: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_3fe5fdd5a604, handleInvalid=error, numInputCols=5

scala> val rf = new RandomForestClassifier().setLabelCol("Survived").setFeaturesCol("features")
     | 
val rf: org.apache.spark.ml.classification.RandomForestClassifier = rfc_13111af94173

scala> val pipeline = new Pipeline().setStages(Array(genderIndexer, assembler, rf))
     | 
val pipeline: org.apache.spark.ml.Pipeline = pipeline_937c02b9a861

scala> val Array(training, validation) = trainDataFinal.randomSplit(Array(0.8, 0.2))
     | 
val training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [PassengerId: int, Survived: int ... 7 more fields]
val validation: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [PassengerId: int, Survived: int ... 7 more fields]

scala> val model = pipeline.fit(training)
     | 
val model: org.apache.spark.ml.PipelineModel = pipeline_937c02b9a861

scala> val predictions = model.transform(validation)
     | 
val predictions: org.apache.spark.sql.DataFrame = [PassengerId: int, Survived: int ... 12 more fields]

scala> val evaluator = new BinaryClassificationEvaluator().setLabelCol("Survived")
     | val accuracy = evaluator.evaluate(predictions)
     | println(s"Accuracy: $accuracy")
Accuracy: 0.8611904761904761
val evaluator: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = BinaryClassificationEvaluator: uid=binEval_593bbbbc24e8, metricName=areaUnderROC, numBins=1000
val accuracy: Double = 0.8611904761904761
